{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification setup and notation\n",
    "\n",
    "* Generally we have a training dataset consisting of samples\n",
    "\n",
    "$${ \\{x_i, y_i \\} }_{i=N}^N$$\n",
    "\n",
    "* $x_i$: inputs - $d$ dimension\n",
    "* $y_i$: labels\n",
    "\n",
    "### Details about softmax\n",
    "\n",
    "$$p(y \\vert x) = \\dfrac{\\exp(W_y \\cdot x)}{\\sum_{c=1}^C \\exp(W_c \\cdot x)}$$\n",
    "\n",
    "seperate into 2 steps:\n",
    "1. Take y'th row of W and multiply that row with x: unnormalized score $f_y$, and compute all $f_c$ for all $c=1, \\cdots ,C$\n",
    "$$W_{y\\cdot} x = \\sum_{i=1}^d W_{yi} x_i = f_y$$\n",
    "2. Normalize!\n",
    "$$p(y \\vert x) = \\dfrac{\\exp(f_y)}{\\sum_{c=1}^C \\exp(f_c)} = softmax(f)_y$$\n",
    "\n",
    "> 1 번에서 W 과 x 의 단순 dot product 가 아니고 W 의 모든 row 와 한번씩 곱해서 summation 한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with softmax and cross-entropy error\n",
    "\n",
    "For each training example $\\{x,y\\}$, our objective is to maximize the probability of the correct class $y$\n",
    "\n",
    "Hence, we minimize the negative log probability of that class. = \"Cross Entropy Error\"\n",
    "\n",
    "**why?**\n",
    "\n",
    "First, since log function is monotonic increase function, so maximizing the probability is same as maximizing log probability.\n",
    "\n",
    "$$\\arg \\max P(y \\vert x) = \\arg \\max \\log P(y \\vert x)$$\n",
    "\n",
    "Second, Cross Entropy.\n",
    "\n",
    "Assuming a ground truth (or gold or target) probability distribution that is 1 at the right class and 0 everywhere else: $p = [0, \\cdots,  0, 1, 0, \\cdots 0]$ and our computed probability is $q$, then the cross entropy is:\n",
    "\n",
    "$$H(p, q) = - \\sum_{c=1}^C p(c)\\log q(c)$$\n",
    "\n",
    "Because of **one-hot p**, the only term left is the negative log probability of the true class\n",
    "\n",
    "In our case, $\\log q(c) = \\log P(y \\vert x)$, also there is \"minus\" at the front, so, $\\arg \\max \\rightarrow \\arg \\min$\n",
    "\n",
    "### Slide note: KL divergence\n",
    "[wiki](https://ko.wikipedia.org/wiki/%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC_%EB%B0%9C%EC%82%B0)\n",
    "\n",
    "Cross-entropy can be re-written in terms of the entropy and Kullback-Leibler divergence between the two distributions:\n",
    "\n",
    "$$H(p, q) = H(p) + D_{KL} (p\\|q)$$\n",
    "\n",
    "Because $H(p)=- \\sum_{c=1}^C p(c)\\log p(c)= -(0\\times \\log 0 + \\cdots + 1 \\times \\log 1 \\cdots )=0$ ( Not evaluate at $p(c)$=0 ) is zero in our case (and even if it wasn’t it would be fixed and have no contribution to gradient), to minimize this is equal to minimizing the KL divergence between $p$ and $q$.\n",
    "\n",
    "The KL divergence is **not a distance** but a non-symmetric measure of the difference between two probability distributions $p$ and $q$\n",
    "\n",
    "$$D_{KL} (p\\|q) = \\sum_{c=1}^C p(c) \\log \\dfrac{p(c)}{q(c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification over a full dataset & Regularization\n",
    "\n",
    "Cross entropy loss function over full dataset ${ \\{x_i, y_i \\} }_{i=N}^N$.\n",
    "\n",
    "$$J(\\theta) = \\dfrac{1}{N} \\sum_{i=1}^N - \\log \\big( \\dfrac{ e^{f_{y_i} } }{ \\sum_{c=1}^C e^{f_c} } \\big)$$\n",
    "\n",
    "Regularization:\n",
    "\n",
    "$$J(\\theta) = \\dfrac{1}{N} \\sum_{i=1}^N - \\log \\big( \\dfrac{ e^{f_{y_i} } }{ \\sum_{c=1}^C e^{f_c} } \\big) + \\lambda \\sum_k \\theta_k^2$$\n",
    "\n",
    "Prevent overfitting when we have a lot of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification difference with word vectors\n",
    "\n",
    "업데이트 할 양이 어마어마하게 많음\n",
    "\n",
    "$$\\triangledown_\\theta J(\\theta) = \n",
    "\\begin{bmatrix} \\triangledown_{W_{\\cdot 1} } \\\\ \\vdots \\\\ \\triangledown_{W_{\\cdot d} } \\\\ \\triangledown_{x_{aardvark} } \\\\ \\vdots \\\\ \\triangledown_{x_{zebra} } \\end{bmatrix} \\in \\Bbb{R}^{Cd + Vd}$$\n",
    "\n",
    "Vd 가 엄청나게 큼, regularization 필요함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A pitfall when retraining word vectors\n",
    "* Losing generalization by re-training word vectors\n",
    "\n",
    "Should I train my own word vectors?\n",
    "* If you only have a small training data set, don’t train the word vectors.\n",
    "* If you have have a very large dataset, it may work better to train word vectors to the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window classification\n",
    "\n",
    "https://arxiv.org/pdf/1103.0398v1.pdf\n",
    "\n",
    "Classifying single words is rarely done. Interesting problems like ambiguity arise in context!\n",
    "\n",
    "즉, context로 부터 이 단어의 의미를 규정할 수 있을때 자주 쓰인다.\n",
    "\n",
    "* Example: auto-antonyms:\n",
    "    * \"To sanction\" can mean \"to permit\" or \"to punish.”\n",
    "    * \"To seed\" can mean \"to place seeds\" or \"to remove seeds.\"\n",
    "\n",
    "* Example: ambiguous named entities:\n",
    "    * Paris $\\rightarrow$ Paris, France vs Paris Hilton\n",
    "    * Hathaway $\\rightarrow$ Berkshire Hathaway vs Anne Hathaway\n",
    "    \n",
    "Idea: classify a word in its context window of neighboring words\n",
    "\n",
    "* Example: Named Entity Recognition is a 4-way classification task:\n",
    "    * Person, Location, Organization, None\n",
    "\n",
    "There are many ways to classify a single word in context\n",
    "* For example: average all the words in a window\n",
    "* Problem: that would lose position information\n",
    "\n",
    "Train softmax classifier to classify a center word by taking concatenation of all word vectors surrounding it\n",
    "\n",
    "* Example: Classify “Paris” in the context of this sentence with window length 2:\n",
    "    * Sentence: ... museums in Paris are amazing ...\n",
    "$$x_{window} = \\begin{bmatrix} x_{museums} & x_{in} & x_{Paris} & x_{are} & x_{amazing} \\end{bmatrix}^T$$\n",
    "    * resulting vector $x_{window} = x \\in R^{5d}$\n",
    "    \n",
    "With $x = x_{window}$ we can use the same softmax classifier as before:\n",
    "\n",
    "$$\\hat{y}_y = p(y \\vert x) = \\dfrac{\\exp(W_y \\cdot x)}{\\sum_{c=1}^C \\exp(W_c \\cdot x)}$$\n",
    "\n",
    "With cross entropy error as before:\n",
    "\n",
    "$$J(\\theta) = \\dfrac{1}{N} \\sum_{i=1}^N - \\log \\big( \\dfrac{ e^{f_{y_i} } }{ \\sum_{c=1}^C e^{f_c} } \\big)$$\n",
    "\n",
    "**Update?** Use derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The max-margin loss\n",
    "\n",
    "* Idea for training objective: Make true window’s score larger and corrupt window’s score lower (until they’re good enough): minimize\n",
    "\n",
    "$$J = max(0, 1-s+s_c)$$\n",
    "\n",
    "* $s$ = score(museums in Paris are amazing)\n",
    "* $s_c$ = score(Not all museums in Paris) = negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
