{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem of navie softmax model\n",
    "\n",
    "The gradient of word vectors $\\triangledown_\\theta J_t(\\theta) \\in \\Bbb{R}^{2dV}$ may be very sparse, because for each window we have at most **2m+1** words to update, but whole vocab($V$) is very a large number.\n",
    "\n",
    "* m: window size\n",
    "\n",
    "많은 양의 단어에 비해 업데이트 하는 파라미터수는 적기 때문에 gradient matrix 가 굉장히 sparse 해질 수 있음. Adam 같은 알고리즘은 sparse 한 matrix 에 취약함. [참고](https://simonjisu.github.io/deeplearning/2018/01/13/numpywithnn_5.html)\n",
    "\n",
    "We may as well only update the word vectors that actually appear! 실제로 등장하는 단어만 업데이트 하자!\n",
    "\n",
    "** Solution: **\n",
    "\n",
    "1. only update certain columns of full embedding matrices $U$ & $V$\n",
    "2. keep around a hash for word vector.\n",
    "\n",
    "paper: Distributed representaions of Words and Phrases and their Compositionality (Mikolov et al. 2013) - [link](https://arxiv.org/abs/1310.4546)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method - [link](https://arxiv.org/abs/1402.3722)\n",
    "\n",
    "Main idea: train binary logistic regressions for a true pair (center word and word in its context window) versus a couple of noise pairs (the center word paired with a random word)\n",
    "\n",
    "**Maximize Overall objective function:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "J(\\theta) &= \\dfrac{1}{T}\\sum_{t=1}^{T} J_t(\\theta)\\\\\n",
    "J(\\theta) &= \\log \\sigma(u_o^T v_c) + \\sum_{i=1}^{k} \\mathbb{E}_{j \\tilde{} P(w)} [\\log \\sigma(-u_j^T v_c)]\n",
    "\\end{aligned}$$\n",
    "\n",
    "* $T$: total num of words\n",
    "* $\\sigma$: sigmoid function\n",
    "* $P(w) = {U(w)^{3/4}} / {Z}$: unigram distribution U(w) raised to the 3/4 power\n",
    "    * The pwer makes less frequent words be sampled mor often\n",
    "\n",
    "     \n",
    "So we maximize the probability of two words co-occurring in the first $\\log \\sigma(u_o^T v_c)$\n",
    "\n",
    "And, sub sample a couple of the words from the corpus (j ~ P(w)), minimize their probability of co-occurring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"word2vec Explained\" 논문 설명(notaion 이 조금 다르니 주의):\n",
    "\n",
    "출발점: (w, c) 세트가 정말로 corpus data로 부터 왔는가? \n",
    "* 여기서 **w = word, c = context** 다.\n",
    "* 정의: \n",
    "    * $P(D = 1|\\ w, c)$ : 단어 (w, c)가 corpus data로부터 왔을 확률\n",
    "    * $P(D = 0|\\ w, c) = 1 - P(D = 1|\\ w, c)$ : 단어 (w, c)가 corpus data로부터 오지 않았을 확률\n",
    "  \n",
    "따라서, 우리의 목적은 확률$P(D = 1|\\ w, c)$를 최대화하는 parameter $\\theta$를 찾는 것이기 때문에 아래와 같은 식을 세울 수 있다.\n",
    "\n",
    "$$\\begin{aligned} &\\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\prod} P(D=1|\\ w,c;\\theta) \\\\\n",
    "= &\\arg \\underset{\\theta}{\\max} \\log \\underset{(w,c) \\in D}{\\prod} P(D=1|\\ w,c;\\theta) \\\\\n",
    "= &\\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\sum} \\log P(D=1|\\ w,c;\\theta)\n",
    "\\end{aligned}$$ \n",
    "\n",
    "파라미터 $\\theta$ 는 단어들의 벡터라고 생각할 수 있다. 즉, 위의 식을 만족하는 어떤 최적의 단어 벡터를 찾는것이다.\n",
    "\n",
    "또한, 확률 $P(D=1|\\ w,c;\\theta)$ 은 sigmoid로 아래와 같이 정의 할 수 있다.\n",
    "\n",
    "$$P(D=1|\\ w,c;\\theta) = \\dfrac{1}{1+e^{-v_c v_w}}$$\n",
    "\n",
    "따라서 우리의 목적은 아래와 같다.\n",
    "\n",
    "$$\\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\sum} \\log \\dfrac{1}{1+e^{-v_c v_w} }$$\n",
    "\n",
    "그러나 우리의 목적 함수는 매 (w, c)마다 $P(D=1|\\ w,c;\\theta)=1$ 를 만족하는 trivial solution이 존재한다. $v_c = v_w$ 이며, $\\forall v_c,\\ v_w$ 에 대해 $v_c \\cdot v_w = K$ 를 만족하는 $\\theta$ (보통 $K$ 가 40이 넘어가면 위 방정식의 값이 0에 가까워짐) 는 모든 값을 똑같이 0으로 만들어 버리기 때문에, 같은 값을 갖지 못하게 하는 매커니즘이 필요하다. ($\\theta$ 에 뭘 넣어도 0이 되면 최대값을 찾는 의미가 없어짐)\n",
    "\n",
    "하나의 방법으로 랜덤 (w, c) 조합을 생성하는 집합 $D'$를 만들어 corpus data로부터 올 확률 $P(D=1|\\ w,c;\\theta)$ 를 낮게 강제하는 것이다. 즉, $D'$ 에서 생성된 (w, c) 조합은 corpus data 로부터 오지 않게 하는 확률 $P(D=0|\\ w,c;\\theta)$ 을 최대화 하는 것.\n",
    "\n",
    "$$\\begin{aligned} \n",
    "& \\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\prod} P(D=1|\\ w,c;\\theta) \\underset{(w,c) \\in D'}{\\prod} P(D=0|\\ w,c;\\theta) \\\\\n",
    "&= \\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\prod} P(D=1|\\ w,c;\\theta) \\underset{(w,c) \\in D'}{\\prod} \\big(1- P(D=1|\\ w,c;\\theta) \\big) \\\\\n",
    "&= \\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\sum} \\log P(D=1|\\ w,c;\\theta) + \\underset{(w,c) \\in D'}{\\sum} \\log \\big(1- P(D=1|\\ w,c;\\theta) \\big) \\\\\n",
    "&= \\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\sum} \\log \\dfrac{1}{1+e^{-v_c v_w} } + \\underset{(w,c) \\in D'}{\\sum} \\log \\big(1- \\dfrac{1}{1+e^{-v_c v_w} } \\big) \\\\\n",
    "&= \\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\sum} \\log \\dfrac{1}{1+e^{-v_c v_w} } + \\underset{(w,c) \\in D'}{\\sum} \\log \\dfrac{1}{1+e^{v_c v_w} }\n",
    "\\end{aligned}$$\n",
    "\n",
    "Let $\\sigma(x) = \\dfrac{1}{1+e^{-x} }$, Then,\n",
    "\n",
    "$$\\begin{aligned} \n",
    "& \\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\sum} \\log \\dfrac{1}{1+e^{-v_c v_w} } + \\underset{(w,c) \\in D'}{\\sum} \\log \\dfrac{1}{1+e^{v_c v_w} } \\\\\n",
    "&= \\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\sum} \\log \\sigma(v_c v_w) + \\underset{(w,c) \\in D'}{\\sum} \\log \\sigma(- v_c v_w)\n",
    "\\end{aligned}$$\n",
    "\n",
    "This equation is similar to equation (4) in Mikolov et al (위에 링크).\n",
    "\n",
    "$$\\log \\sigma(u_c^T v_w) + \\sum_{i=1}^{k} \\mathbb{E}_{j \\tilde{} P(w)} [\\log \\sigma(-u_j^T v_w)]$$\n",
    "\n",
    "다른 점이라면, 우리가 만든 식에서는 전체 corpus ($D \\cup D'$) 을 포함하지만, Mikolov 논문의 식은 D 에 속하는 (w, c) 조합 하나와 k 개의 다른 (w, c_j) 의 조합을 들었다는 것이다. 구체적으로, k 번의 negative sampling 에서 Mikolov 는 $D'$ 를 $k \\times D$ 보다 크게 설정했고, k개의 샘플 $(w, c_1), (w, c_2), \\cdots, (w, c_k)$ 에 대해서 $c_j$ 는 **unigram distribution** 에 **3/4** 승으로 부터 도출된다. 이는 아래의 분포에서 (w, c) 조합을 추출 하는 것과 같다.\n",
    "\n",
    "$$p_{words}(w) = \\dfrac{p_{contexts} (c)^{3/4} }{Z}$$\n",
    "\n",
    "* $p_{words}(w)$, $p_{contexts} (c)$ 는 각각 words and contexts 의 unigram distribution 이다.\n",
    "* $Z$ 는 normalization constant\n",
    "\n",
    "Mikolov 논문에서는 context는 하나의 단어이기 때문에 $p_{words}(w) = p_{contexts} (c) = \\dfrac{count(x)}{ | text | }$\n",
    "\n",
    "#### 참고\n",
    "* trivial solution: 모든 해가 0 \n",
    "* non-trivial solution: 무수히 많은 해가 있음(보통 하나가 free variable)\n",
    "* homogenous, trival solution 개념 설명: https://www.youtube.com/watch?v=JlJWyWJARRU\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여태까지 하려던 것: captures cooccurrence of words by one at a time\n",
    "\n",
    "But, Why not capture cooccurrence counts directly?\n",
    "\n",
    "**2 Options**\n",
    "* window: similar to word2vec, captrues both syntactic (POS) and semantic information\n",
    "* word-document co-occurrence matrix: give general topics, leading to \"Latent Semantic Analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Window based co-occurrence matrix\n",
    "\n",
    "* Window length = 1 \n",
    "* Symmetric (irrelevant whether left or right, can be unsymmetric but in this case yes)\n",
    "* Example:\n",
    "    * I like deep learning.\n",
    "    * I like NLP.\n",
    "    * I enjoy flying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|counts| I | like | enjoy | deep | learning | NLP | flying | . |\n",
    "|--|--|--|--|--|--|--|--|--|\n",
    "| I| 0 | 2 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
    "|like| 2 | 0 | 0 | 1 | 0 | 1 | 0 | 0 |\n",
    "|enjoy| 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |\n",
    "|deep| 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
    "|learning| 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 |\n",
    "|NLP| 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
    "|flying| 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 |\n",
    "|.| 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'deep', 'I', 'like', 'NLP', 'learning', '.', 'flying']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque, Counter\n",
    "from itertools import islice\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "sentences = ['I like deep learning .', 'I like NLP .', 'I enjoy flying .']\n",
    "tokens = [s.split() for s in sentences]\n",
    "vocab = list(set([w for s in tokens for w in s]))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatten = lambda t: [tuple(j) for i in t for j in i]\n",
    "\n",
    "def get_cooccur_list(sentence, window):\n",
    "    s_len = len(sentence)\n",
    "    ngram_list = [deque(islice(sentence, i), window+1) for i in range(s_len+1)][2:]\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = ['I', 'like', 'enjoy', 'deep', 'learning', 'NLP', 'flying', '.']\n",
    "vocab2idx = {w: i for i, w in enumerate(vocab)}\n",
    "tokens_idx = [[vocab2idx.get(w) for w in s] for s in tokens] \n",
    "window = 1\n",
    "co_occurs = [get_cooccur_list(s, window) for s in tokens_idx]\n",
    "d = Counter()\n",
    "d.update(flatten(co_occurs))\n",
    "row, col, data = list(zip(*[[r, c, v] for (r, c), v in d.items()]))\n",
    "temp = coo_matrix((data, (row, col)), shape=(len(vocab), len(vocab))).toarray()\n",
    "co_mat = temp.T + temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>like</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>deep</th>\n",
       "      <th>learning</th>\n",
       "      <th>NLP</th>\n",
       "      <th>flying</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enjoy</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flying</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I  like  enjoy  deep  learning  NLP  flying  .\n",
       "I         0     2      1     0         0    0       0  0\n",
       "like      2     0      0     1         0    1       0  0\n",
       "enjoy     1     0      0     0         0    0       1  0\n",
       "deep      0     1      0     0         1    0       0  0\n",
       "learning  0     0      0     1         0    0       0  1\n",
       "NLP       0     1      0     0         0    0       0  1\n",
       "flying    0     0      1     0         0    0       0  1\n",
       ".         0     0      0     0         1    1       1  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(co_mat, index=vocab, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with simple co-occurrence vectors:\n",
    "* Increase in size with vocabulary\n",
    "* Very high dimensional: require a lot of storage\n",
    "* Subsequent classification models have sparsity issues $\\rightarrow$ models are less robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions: Low dimensional vectors\n",
    "Idea: store \"most\" of the important information in a fixed, small number of dimensions - a dense vector\n",
    "\n",
    "How to reduce dimentionality??\n",
    "\n",
    "### Method 1: SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U, s, Vh = np.linalg.svd(co_mat, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAG9CAYAAABksG0oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nigAAOAhFAAAchCIAAI7/AToQeWxRGo85AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1076b470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "for i in range(len(vocab)):\n",
    "    ax.annotate(vocab[i], (U[i, 0], U[i, 1]))\n",
    "ax.grid(False)\n",
    "ax.set_xlim(-1.2, 0.7)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_title('reduce dimentionality using SVD', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacks to X\n",
    "* Problem: function words (the, he, has) are too frequent $\\rightarrow$ syntax has too much impact\n",
    "    * Solution: fixes count or ignore all\n",
    "* Ramped windows that count closer words more\n",
    "* Use Pearson correlations instead of counts, then set negative values to 0\n",
    "\n",
    "\n",
    "An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence Rohde et al. 2005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with SVD\n",
    "Computational cost scales quadratically for n x m matrix: $O(mn^2)$ flops (when n < m) \n",
    "\n",
    "$\\rightarrow$ Bad for millions of words or documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count based vs Direct prediction\n",
    "\n",
    "|Count based |Direct prediction|\n",
    "|--|--|\n",
    "|LSA, HAL (Lund & Burgess), <br> COALS, Hellinger-PCA (Rohde et al, Lebret & Collobert) | Skip-gram/CBOW (Mikolov et al) <br> NNLM, HLBL, RNN (Bengio et al; Collobert & Weston; Huang et al; Mnih & Hinton) |\n",
    "|(G)Fast training <br> (G)Efficient usage of statistics <br> (B)Primarily used to capture wordsimilarity <br>  (B)Disproportionate importance given to large counts | (B)Scale with corpus size <br> (B)Inefficient usage of statistics <br> (G)Generate improved performance on other tasks <br> (G)Can capture complex patterns beyond word similarity|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\dfrac{1}{2} \\sum_{i,j=1}^{W} f(P_{ij})(u_i^T v_j - \\log P_{ij})^2$$\n",
    "\n",
    "Good Points(By Pennington, Socher, Manning (2014))\n",
    "* Fast Training\n",
    "* Scalable to huge corpora\n",
    "* Good Performance even with small corpus and small vectors\n",
    "\n",
    "Paper: [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Glove Paper explain\n",
    "\n",
    "**GloVe:** Global Vectors, because the global corpus statistics are captured directly by the model\n",
    "\n",
    "Define notation:\n",
    "* $X$: matrix of word-word co-occurrence counts\n",
    "* $X_{ij}$: the number of times that word $j$ occurs in the context word $i$\n",
    "* $X_i = \\sum_k X_{ik}$: the number of times any word appears in the context of word $i$\n",
    "* $P_{ij} = P(j | i) = X_{ij} / X_i$: probability that word $j$ appear in the context word $i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "* I like deep learning.\n",
    "* I like NLP.\n",
    "* I enjoy flying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = (co_mat / co_mat.sum(axis=1).reshape(-1, 1))\n",
    "cols = ['k='+v for v in vocab]\n",
    "rows = ['p(k|{})'.format(v) for v in vocab]\n",
    "P = pd.DataFrame(data, index=rows, columns=cols) + 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k=I</th>\n",
       "      <th>k=like</th>\n",
       "      <th>k=enjoy</th>\n",
       "      <th>k=deep</th>\n",
       "      <th>k=learning</th>\n",
       "      <th>k=NLP</th>\n",
       "      <th>k=flying</th>\n",
       "      <th>k=.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p(k|I)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p(k|like)</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p(k|enjoy)</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p(k|deep)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p(k|learning)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p(k|NLP)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p(k|flying)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p(k|.)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               k=I  k=like  k=enjoy  k=deep  k=learning  k=NLP  k=flying  k=.\n",
       "p(k|I)         0.0    0.67     0.33    0.00        0.00   0.00      0.00  0.0\n",
       "p(k|like)      0.5    0.00     0.00    0.25        0.00   0.25      0.00  0.0\n",
       "p(k|enjoy)     0.5    0.00     0.00    0.00        0.00   0.00      0.50  0.0\n",
       "p(k|deep)      0.0    0.50     0.00    0.00        0.50   0.00      0.00  0.0\n",
       "p(k|learning)  0.0    0.00     0.00    0.50        0.00   0.00      0.00  0.5\n",
       "p(k|NLP)       0.0    0.50     0.00    0.00        0.00   0.00      0.00  0.5\n",
       "p(k|flying)    0.0    0.00     0.50    0.00        0.00   0.00      0.00  0.5\n",
       "p(k|.)         0.0    0.00     0.00    0.00        0.33   0.33      0.33  0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$i$ = deep, $j$ = NLP. When $k$ = learning, we expect $P_{ik}/P_{jk}$ be very lagre, vice versa. When $k$ = like, expect $P_{ik}/P_{jk}$ should be close to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000000001.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P_ij, i=deep / j=NLP\n",
    "P.loc['p(k|deep)','k=learning'] / P.loc['p(k|NLP)','k=learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9999999996e-10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P_ij, i=NLP / j=deep\n",
    "P.loc['p(k|NLP)','k=learning'] / P.loc['p(k|deep)','k=learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P_ij, i=learning / j=NLP\n",
    "P.loc['p(k|deep)','k=like'] / P.loc['p(k|NLP)','k=like']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above argument suggests that the appropriate starting point for word vector learning should be with ratios of co-occurrence probabilities rather than the probabilities themselves.\n",
    "\n",
    "Noting that the ratio $P_{ik} /P_{jk}$ depends on three words $i$, $j$, and $k$, the most general model takes the form,\n",
    "\n",
    "$$F(w_i, w_j, \\tilde{w}_k) = \\dfrac{P_{ik} }{P_{jk} } \\cdots (1)$$\n",
    "\n",
    "* $w \\in \\Bbb{R}^d$: word vectors\n",
    "* $\\tilde{w} \\in \\Bbb{R}^d$: separate context word vectors\n",
    "\n",
    "Expect finding a linear structure function $F$ where inputs are $w_i, w_j, \\tilde{w}_k$ to encode the information present the ratio $\\dfrac{P_{ik} }{P_{jk} }$ in the word vector space.\n",
    "\n",
    "Let $F$ be $F((w_i - w_j)^T \\tilde{w}_k) = \\dfrac{P_{ik} }{P_{jk} } \\cdots (2)$, so that both side of equation can be a scalar value. Also, can try to capture the linear relation between $w_i, w_j$ and $\\tilde{w}_k$\n",
    "\n",
    "Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles. To do so consistently, we must not only exchange $w \\leftrightarrow \\tilde{w}$, but also $X \\leftrightarrow X^T$. + $F(X-Y) = F(X) / F(Y)$\n",
    "\n",
    "Final model must contains these.\n",
    "\n",
    "Symmetry:\n",
    "\n",
    "First, F be a **homomorphism** between groups $(\\Bbb{R}, +)$ and $(\\Bbb{R}_{>0}, \\times)$,\n",
    "\n",
    "$$F(w_i, w_j, \\tilde{w}_k) = \\dfrac{F(w_i^T \\tilde{w}_k) }{F(w_j^T \\tilde{w}_k) } \\cdots (3)$$\n",
    "    \n",
    "by equation $(2)$ is solved by, \n",
    "\n",
    "$$F(w_i^T \\tilde{w}_k) = P_{ik} = \\dfrac{X_{ik} }{X_i} \\cdots (4)$$\n",
    "\n",
    "The solution for equation $(4)$ is $F = \\exp$, so,\n",
    "\n",
    "$$w_i^T \\tilde{w}_k = \\log(P_{ik}) = \\log(X_{ik}) - \\log(X_i) \\cdots (5)$$\n",
    "\n",
    "Next, $(5)$ will be symmetric if not $\\log(X_i)$. Let's see is $\\log(P_{ik})=\\log(P_{ki})$\n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\log(P_{ik}) &= \\log(X_{ik}) - \\log(X_i) \\\\\n",
    "\\log(P_{ki}) &= \\log(X_{ki}) - \\log(X_k)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Obiously, term $\\log(X_i) \\neq \\log(X_k)$ makes $\\log(P_{ik}) \\neq \\log(P_{ki})$\n",
    "\n",
    "However $\\log(X_i)$ term is independent of $k$ so it can be absorbed into a bias $b_i$ for $w_i$.\n",
    "\n",
    "Finally adding an additional bias $\\tilde{b}_k$ for $\\tilde{w}_k$ restores the symmetry.\n",
    "\n",
    "$$w_i^T \\tilde{w}_k + b_i + \\tilde{b}_k = \\log(X_{ik}) \\cdots (6)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
