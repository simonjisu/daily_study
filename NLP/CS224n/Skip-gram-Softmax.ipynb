{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec: Skip-Gram-Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper: Efficient Estimation of Word Representations in Vector Space\n",
    "https://arxiv.org/pdf/1301.3781.pdf\n",
    "\n",
    "paper: Distributed Representations of Words and Phrases and their Compositionality\n",
    "\n",
    "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "\n",
    "Word2Vec: 딥러닝을 활용한 단어를 벡터화 하는 하나의 방법이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![w2v.png](./figs/w2v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence: The quick brown fox jumps over the lazy dogs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skipgram](./figs/skip-gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input: one-hot Vector로 된 단어 size = (V,)\n",
    "    * I $\\rightarrow$ H : $W_{(V, N)}$\n",
    "* Hidden: Projection 된 히든층 size = (N,)\n",
    "    * H $\\rightarrow$ O : $W'_{(N, V)}$\n",
    "* Output: 전후로 나오는 단어의 확률들\n",
    "\n",
    "$H =  W^T \\cdot X $\n",
    "\n",
    "$O = (W')^T \\cdot H$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned} \\min J(\\theta) &= -\\dfrac{1}{T} \\sum_{t=1}^T \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\log P(w_{t+j} | w_t) \\\\\n",
    "P(o|c) &= \\dfrac{\\exp(u_o^T V_c)}{\\sum_{w=1}^V \\exp(u_w^T V_c)}\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f = \\log \\dfrac{\\exp(u_o^T V_c)}{\\sum_{w=1}^V \\exp(u_w^T V_c)}$$\n",
    "\n",
    "$$\\begin{aligned} \\dfrac{\\partial f}{\\partial V_c} \n",
    "&= \\dfrac{\\partial }{\\partial V_c} \\big(\\log(\\exp(u_o^T V_c)) - \\log(\\sum_{w=1}^V \\exp(u_w^T V_c))\\big) \\\\\n",
    "&= u_o - \\dfrac{1}{\\sum_{w=1}^V \\exp(u_w^T V_c)}(\\sum_{x=1}^V \\exp(u_x^T V_c) u_x ) \\\\\n",
    "&= u_o - \\sum_{x=1}^V \\dfrac{\\exp(u_x^T V_c)}{\\sum_{w=1}^V \\exp(u_w^T V_c)} u_x \\\\ \n",
    "&= u_o - \\sum_{x=1}^V P(x | c) u_x\n",
    "\\end{aligned}$$\n",
    "\n",
    "* $u_o$ : observed word, output contextword\n",
    "* $P(x|c)$: probs x given word context c  \n",
    "* $P(x|c)u_x$: Expectation of all the context word: likelihood occurance probs $\\times$ context vector  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c0c65f8d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0.post4\n",
      "3.2.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.gutenberg.sents('melville-moby_dick.txt')[:100]\n",
    "corpus = [[word.replace('\"', \"'\").lower() for word in sent] for sent in corpus]\n",
    "# corpus = [[w for word in sent for w in word_tokenize(word.replace('\"', \"'\").lower())] for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class corpus_process(object):\n",
    "    def __init__(self, drop_rate=0.01, min_count=3):\n",
    "        self.vocab2idx = defaultdict()\n",
    "        self.vocab2idx['<NULL>'] = 0\n",
    "        self.vocab_counts = Counter()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.min_count = min_count\n",
    "        self.stopwords = None\n",
    "        self.stop_min_count = []\n",
    "        self.flatten = lambda l: [item for sub in l for item in sub]\n",
    "        self.V = None\n",
    "        self.vocab = None\n",
    "        \n",
    "    def drop_words(self, corpus):\n",
    "        \"\"\"drop words by unigram distribution's tails\"\"\"\n",
    "        self.vocab_counts.update(self.flatten(corpus))\n",
    "        border = int(len(self.vocab_counts) * self.drop_rate)\n",
    "        stops = self.vocab_counts.most_common()[:border] + \\\n",
    "                        list(reversed(self.vocab_counts.most_common()))[:border]\n",
    "        self.stopwords = [s[0] for s in stops]\n",
    "        \n",
    "        for w, c in self.vocab_counts.items():\n",
    "            if c < self.min_count:\n",
    "                self.stop_min_count.append(w)\n",
    "        \n",
    "    def fit(self, corpus):\n",
    "        self.drop_words(corpus)\n",
    "        vocab = list(set(self.flatten(corpus)) - set(self.stopwords))\n",
    "        for i, word in enumerate(vocab, 1):\n",
    "            self.vocab2idx[word] = i\n",
    "        self.idx2vocab = {v: k for k, v in self.vocab2idx.items()}\n",
    "        vocab.append('<NULL>')\n",
    "        self.vocab = vocab\n",
    "        self.V = len(vocab)\n",
    "\n",
    "    def transform_word(self, word):\n",
    "        return self.vocab2idx['<NULL>'] if self.vocab2idx.get(word) is None else self.vocab2idx[word]   \n",
    "    \n",
    "    def transform_seq(self, seq):\n",
    "        idxs = list(map(lambda w: self.vocab2idx[\"<NULL>\"] if self.vocab2idx.get(w) is None else self.vocab2idx[w], seq))\n",
    "        return torch.LongTensor(idxs)\n",
    "    \n",
    "    def transform2data(self, corpus, win_size=2):\n",
    "        \"\"\"transform words to idx adding pad data\"\"\"\n",
    "        total_data = self.flatten([list(nltk.ngrams(['<NULL>']*win_size + c + ['<NULL>']*win_size, 2 * win_size + 1)) for c in corpus])\n",
    "        datas = []\n",
    "        for data in total_data:\n",
    "            for i in range(win_size*2 + 1):\n",
    "                if data[i] == '<NULL>' or i == win_size:\n",
    "                    continue\n",
    "                datas.append( (self.transform_word(data[win_size]),\n",
    "                               self.transform_word(data[i])) )\n",
    "        return np.array(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CP = corpus_process(drop_rate=0.001, min_count=3)\n",
    "CP.fit(corpus)\n",
    "datas = CP.transform2data(corpus, win_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5258, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = CP.V  # VOCAB SIZE\n",
    "N = 100  # EMBEDDING SIZE\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 200\n",
    "datas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torchdata.Dataset):\n",
    "    def __init__(self, data, transform=True):\n",
    "\n",
    "        self.x = torch.LongTensor(data[:, 0]).contiguous().view(-1, 1)\n",
    "        self.y = torch.LongTensor(data[:, 1]).contiguous().view(-1, 1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 인덱스에 해당하는 데이터셋 리턴\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 데이터셋 수\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mydataset = CustomDataset(data=datas)\n",
    "data_loader = torchdata.DataLoader(dataset=mydataset,\n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   shuffle=False, \n",
    "                                   drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, V, N):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        self.V = V  # Vocab size\n",
    "        self.N = N  # projection size\n",
    "\n",
    "        self.embed_v = nn.Embedding(self.V, self.N, sparse=True)\n",
    "        self.embed_u = nn.Embedding(self.V, self.N, sparse=True)\n",
    "        self._weight_init()\n",
    "        \n",
    "    def _weight_init(self):\n",
    "        initrange = 0.5 / self.N\n",
    "        self.embed_v.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embed_u.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "    def forward(self, c, t, o):\n",
    "        # c: center words\n",
    "        # t: target words\n",
    "        # o: output words = vocabs\n",
    "        c_embeds = self.embed_v(c) # B x 1 x N\n",
    "        t_embeds = self.embed_u(t) # B x 1 x N\n",
    "        o_embeds = self.embed_u(o) # B x V x N\n",
    "        \n",
    "        # each batch scores: we can do this because of simple linear model\n",
    "        scores = t_embeds.bmm(c_embeds.transpose(1, 2)).squeeze(2) # Bx1xN * BxNx1 => Bx1\n",
    "        norm_scores = o_embeds.bmm(c_embeds.transpose(1, 2)).squeeze(2) # BxVxN * BxNx1 => BxV\n",
    "        # log-softmax\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) \n",
    "        \n",
    "        return nll # negative log likelihood\n",
    "    \n",
    "    def predict(self, x):\n",
    "        embeds = self.embed_v(x)\n",
    "        \n",
    "        return embeds         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WV = Word2Vec(V, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SparseAdam(WV.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 6.38\n",
      "Epoch : 10, mean_loss : 5.88\n",
      "Epoch : 20, mean_loss : 5.52\n",
      "Epoch : 30, mean_loss : 5.23\n",
      "Epoch : 40, mean_loss : 5.00\n",
      "Epoch : 50, mean_loss : 4.77\n",
      "Epoch : 60, mean_loss : 4.51\n",
      "Epoch : 70, mean_loss : 4.26\n",
      "Epoch : 80, mean_loss : 4.03\n",
      "Epoch : 90, mean_loss : 3.81\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    total_loss = []\n",
    "    for batch_x, batch_y in data_loader:\n",
    "        inputs, targets = Variable(batch_x), Variable(batch_y) # Bx1\n",
    "        vocabs = Variable(CP.transform_seq(list(CP.vocab)).expand(inputs.size(0), V)) # BxV\n",
    "        WV.zero_grad()\n",
    "        \n",
    "        loss = WV.forward(inputs, targets, vocabs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss.data[0])\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch : {0}, mean_loss : {1:.2f}'.format(epoch, np.mean(total_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.random.choice(CP.vocab)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_similarity(word, topn=10):\n",
    "    idx = Variable(torch.LongTensor([CP.transform_word(word)]))\n",
    "    word_vec = WV.predict(idx)\n",
    "    sims = []\n",
    "    for i in range(CP.V):\n",
    "        if CP.vocab[i] == word: continue\n",
    "        sim_idx = Variable(torch.LongTensor([CP.transform_word(CP.vocab[i])]))\n",
    "        vec = WV.predict(sim_idx)\n",
    "        cosine_sim = F.cosine_similarity(word_vec, vec).data[0] \n",
    "        sims.append([CP.vocab[i], cosine_sim])\n",
    "    return sorted(sims, key=lambda x: x[1], reverse=True)[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_similarity(test, topn=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
