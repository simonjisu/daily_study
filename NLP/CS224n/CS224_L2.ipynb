{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* author: simonjisu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we represent the meaning of a word?\n",
    "\n",
    "Definition: **meaning** (webster dictionary)\n",
    "\n",
    "* the idea that is represented by a word, phrase, etc.\n",
    "* the idea that a person wants to express by using words, signs, etc.\n",
    "* the idea that is expressed in a work of writing, art, etc.\n",
    "\n",
    "Commononest linguistic way of thinking of meaning:\n",
    "* signifier(기표) $\\Longleftrightarrow$ signified(기의) (idea or thing) = denotation(표시, 명시적의미)\n",
    "    - signifier: 시니피앙(언어가 소리와 그 소리로 표시되는 의미로 성립된다고 할 때, 소리를 가리킴)\n",
    "    - signified: 시니피에(언어가 소리와 그 소리로 표시되는 의미로 성립된다고 할 때, 의미를 가리킴) \n",
    "\n",
    "국어사전에서의 **의미** 정의\n",
    "\n",
    "1. 말이나 글의 뜻.\n",
    "    * 단어의 사전적 의미\n",
    "    * 문장의 의미\n",
    "    * 두 단어는 같은 의미로 쓰인다.\t \n",
    "2. 행위나 현상이 지닌 뜻.\n",
    "    * 삶의 의미\n",
    "    * 역사적 의미\n",
    "    * 의미 있는 웃음\t \n",
    "3. 사물이나 현상의 가치.\n",
    "    * 의미 있는 삶을 살다\n",
    "    * 여가를 의미 있게 보내다.\n",
    "    * 의미 없는 행동\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we have usable meaning in a computer?\n",
    "\n",
    "Common answer: Use a taxonomy(분류) like WordNet that has hypernyms(상위어)(is-a) relationships and synonym(동의어) sets\n",
    "\n",
    "English: wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "panda = wn.synset('panda.n.01')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with this discrete representation\n",
    "* Great as a resource but missing nuances(의미・소리・색상・감정상의 미묘한 차이, 뉘앙스), e.g., **synonyms**: 단어간의 미묘한 차이를 넣을 수 없음\n",
    "    * adept, expert, good, practiced, proficient, skillful\n",
    "    * ex) i'm good (vs expert) at deeplearning\n",
    "* Missing new words (impossible to keep up to date): 매일같이 업데이트 불가능(비용이 너무큼)\n",
    "* Subjective: 사람마다 다름, 주관적임\n",
    "* Requires human labor to create and adapt: 사람 손을 많이 탐\n",
    "* Hard to compute accurate word similarity: 유사도 계산이 어려움\n",
    "\n",
    "The vast majority of rele-based and statisical NLP work regards words as atomic symbols: **one-hot-representation**\n",
    "$$word = [0, 0, 0, 1, 0, 0, 0]$$\n",
    "\n",
    "BAD because: \n",
    "* Dimensionality: too long when there are a lot lot of words 단어가 많아 질 수록 너무길어짐\n",
    "* Localist representation: Doesn't give inherent notion, independent for each word, which means cannot calculate similarity 단어의 내적의미를 포함하지 않음, 독립적임(즉, 사람눈에 유사한 단어라도 기계입장에서는 다른 단어일 뿐)\n",
    "    * when someone want to find \"Seattle motel\", we have to match and give him \"Seattle motel\"\n",
    "    * **orthogonal**\n",
    "$$\\begin{aligned} \n",
    "motel &= \\begin{bmatrix} 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\end{bmatrix} \\\\\n",
    "hotel &= \\begin{bmatrix} 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\end{bmatrix} \\\\\n",
    "\\end{aligned}$$\n",
    "$$hotel \\cdot motel^T = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional similarity based representations\n",
    "\n",
    "You can get a lot of value by representing a word by means of its neighbors\n",
    "\n",
    "* When a word $w$ appears in a text, its **context** is the set of words that appear nearby (within a fixed-size window). 단어 $w$ 주변에 나타나는 단어들을 **맥락(문맥)** 이라고함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "Build a dense vector for each word type, chosen so that it is good at predicting other words appearing in its context. 문맥에서 비슷한 단어들을 잘 예측 될 수 있게 단어 타입 별로 촘촘한 벡터(0이 별로없는)를 만든다.\n",
    "\n",
    "그러나 이러한 word vector 가 단어의 개념을 뜻하는 것은 아님, 단지 분포상에서의 의미(distributional meaning)를 뜻함 \n",
    "\n",
    "Idea:\n",
    "* We have a large corpus of text\n",
    "* Every word in a fixed vocabulary is represented by a vector\n",
    "* Go through each **position** $t$ in the text, which has a **center word** $c$ and **context (\"outside\") words** $o$\n",
    "* Use the similarity of the word vectors for $c$ and $o$ to calculate the probability of $o$ given $c$ (or vice versa)\n",
    "* Keep adjusting the word vectors to maximize this probability\n",
    "\n",
    "요약: 모든 단어를 대상으로, 중심단어 $c$ 가 주어졌을 때 그 주변 단어 $o$ 를 나오게 하는 하나의 확률분포을 최대화 시킴 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Example:\n",
    "\n",
    "$$\\begin{bmatrix} 1&4&7&10 \\\\ 2&5&8&11 \\\\ 3&6&9&12 \\end{bmatrix} \\begin{bmatrix} 1\\\\0\\\\0\\\\0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "get first column of matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "[Word2vec](https://arxiv.org/pdf/1310.4546.pdf) (Mikolov et al. 2013)\n",
    "\n",
    "Only one probability distribution of a center word\n",
    "\n",
    "Two Algorithms\n",
    "1. Skip-grams(SG)\n",
    "    * Predict context words given target (position independent)\n",
    "2. Continuous Bag of Words (CBOW)\n",
    "    * Predict target word from bag-of-words context\n",
    "\n",
    "Two (moderately efficient) training methods (vs Naive Softmax)\n",
    "1. Hierarchical softmax\n",
    "2. Negative sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propose\n",
    "for each position of word $c$:\n",
    "\n",
    "$$\\max J(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} p(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "change it to negative log likelihood:\n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\min J(\\theta) &= -\\dfrac{1}{T} \\sum_{t=1}^T \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\log P(w_{t+j} | w_t) \\\\\n",
    "P(o|c) &= \\dfrac{\\exp(u_o^T V_c)}{\\sum_{w=1}^V \\exp(u_w^T V_c)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "* 왜 nll 로 바꾸는 것인가?  \n",
    "    * https://ratsgo.github.io/statistics/2017/09/22/information/\n",
    "    * https://ratsgo.github.io/statistics/2017/09/23/MLE/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product & Softmax\n",
    "\n",
    "* Dot product: similar to calcuate similarity\n",
    "\n",
    "$$u^Tv = u\\cdot v = \\sum_i u_i v_i$$\n",
    "\n",
    "* Softmax\n",
    "\n",
    "$$softmax(x_i) = \\dfrac{\\exp(x_i)}{\\sum_{j=1}\\exp(x_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train: Compute all vector gradients\n",
    "* define the set of all parameters in a model in terms of one long vector $\\theta \\in \\Bbb{R}^{2dV}$\n",
    "* why $2dV$? Because for each word there is a vector as a center word($c$) and as a context word($o$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L2_Skipgram](./figs/L2_Skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update?\n",
    "\n",
    "$$f = \\log \\dfrac{\\exp(u_o^T V_c)}{\\sum_{w=1}^V \\exp(u_w^T V_c)}$$\n",
    "\n",
    "$$\\begin{aligned} \\dfrac{\\partial f}{\\partial V_c} \n",
    "&= \\dfrac{\\partial }{\\partial V_c} \\big(\\log(\\exp(u_o^T V_c)) - \\log(\\sum_{w=1}^V \\exp(u_w^T V_c))\\big) \\\\\n",
    "&= u_o - \\dfrac{1}{\\sum_{w=1}^V \\exp(u_w^T V_c)}(\\sum_{x=1}^V \\exp(u_x^T V_c) u_x ) \\\\\n",
    "&= u_o - \\sum_{x=1}^V \\dfrac{\\exp(u_x^T V_c)}{\\sum_{w=1}^V \\exp(u_w^T V_c)} u_x \\\\ \n",
    "&= u_o - \\sum_{x=1}^V P(x | c) u_x\n",
    "\\end{aligned}$$\n",
    "\n",
    "* $u_o$ : observed word, output context word\n",
    "* $P(x|c)$: probs context word $x$ given center word $c$  \n",
    "* $P(x|c)u_x$: Expectation of all the context words: likelihood occurance probs $\\times$ center vector  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
