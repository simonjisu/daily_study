{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax with Negative Sampling\n",
    "\n",
    "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method\n",
    "* https://arxiv.org/abs/1402.3722"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMIND\n",
    "\n",
    "## softmax\n",
    "$$P(o|c) = \\dfrac{\\exp(u_o^T v_c)}{\\sum_{w=1}^V \\exp(u_w^T v_c)}$$\n",
    "* propose: maximize $P(o|c)$\n",
    "* Cost function is not convex, initialization is not matter\n",
    "* update: large and sparse\n",
    "    * we want to update the word vectors that actually appear! \n",
    "    * --> how to solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "1. need sparse matrix update operations to only update certain columns of full embedding matrices U and V\n",
    "2. nedd to keep around a hash for word vectors\n",
    "\n",
    "paper: Distributed representaions of Words and Phrases and their Compositionality (Mikolov et al. 2013)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "Main idea: train binary logistic regressions for a true pair (center word and word in its context window) versus a couple of noise pairs (the center word paired with a random word)\n",
    "\n",
    "* Maximize Overall objective function:\n",
    "$$\\begin{aligned}\n",
    "J(\\theta) &= \\dfrac{1}{T}\\sum_{t=1}^{T} J_t(\\theta)\\\\\n",
    "J(\\theta) &= \\log \\sigma(u_o^T v_c) + \\sum_{i=1}^{k} \\mathbb{E}_{j \\tilde{} P(w)} [\\log \\sigma(-u_j^T v_c)]\n",
    "\\end{aligned}$$\n",
    "    * $T$: total num of words\n",
    "    * $\\sigma$: sigmoid function\n",
    "    * $P(w) = {U(w)^{3/4}} / {Z}$: unigram distribution U(w) raised to the 3/4 power\n",
    "    \n",
    "So we maximize the probability of two words co-occurring in the first $\\log \\sigma(u_o^T v_c)$\n",
    "\n",
    "And, sub sample a couple of the words from the corpus (j ~ P(w)), minimize their probability of co-occurring\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논문 설명:\n",
    "* 출발점: (w, c) 세트가 정말로 corpus data로 부터 왔는가? \n",
    "* 정의: \n",
    "    * $P(D = 1|w, c)$ (w, c)가 corpus data로부터 왔을 확률\n",
    "    * $P(D = 0|w, c) = 1 - P(D = 1|w, c)$ (w, c)가 corpus data로부터 오지 않았을 확률\n",
    "  \n",
    "따라서, 우리의 목적은 확률$P(D = 1|w, c)$를 최대화하는 parameter $\\theta$를 찾는 것이기 때문에 아래와 같은 식을 세울 수 있다.\n",
    "$$\\begin{aligned} &\\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\prod} P(D=1|w,c;\\theta) \\\\\n",
    "= &\\arg \\underset{\\theta}{\\max} \\log \\underset{(w,c) \\in D}{\\prod} P(D=1|w,c;\\theta) \\\\\n",
    "= &\\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\sum} \\log P(D=1|w,c;\\theta)\n",
    "\\end{aligned}$$ \n",
    "\n",
    "확률 $P(D=1|w,c;\\theta)$은 sigmoid로 아래와 같이 정의 할 수 있다.\n",
    "$$P(D=1|w,c;\\theta) = \\dfrac{1}{1+e^{-v_c v_w}}$$\n",
    "\n",
    "따라서 우리의 목적은 아래와 같다.\n",
    "$$\\arg \\underset{\\theta}{\\max} \\underset{(w,c) \\in D}{\\sum} \\log \\dfrac{1}{1+e^{-v_c v_w}}$$\n",
    "\n",
    "그러나 우리의 목적 함수는 trivial solution이 존재한다. \n",
    "\n",
    "#### 참고\n",
    "trivial solution: 모든 해가 0 이어야 방정식이 풀리는거\n",
    "non-trivial solution: 무수히 많은 해가 있음(보통 하나가 free variable)\n",
    "homogenous, trival solution 개념 설명: https://www.youtube.com/watch?v=JlJWyWJARRU\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "torch.manual_seed(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
